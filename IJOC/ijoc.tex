%\magnification=1000
\hsize=36truepc \vsize=55truepc 
\parindent=18truept \parskip=0pt 
\baselineskip=12pt plus 2pt minus 1pt \lineskiplimit=2pt
\lineskip=2pt plus 2pt \tolerance=800

\font\rmeight=cmr8  \def\rmVIII{\rmeight \baselineskip=10pt
 plus 2pt minus 1pt \lineskiplimit=2pt \lineskip=2pt plus 2pt
 \def\sstrut{\vrule height 7.5pt depth 2.5pt width 0pt}\parskip=0pt}
\font\caps=cmcsc10 \font\slIX=cmsl9
\font\rmIX=cmr9 \font\bfIX=cmbx9 \font\rmXII=cmr12 \font\bfXII=cmbx12
\font\bfXIV=cmbx12 at 14pt
\font\rmXIV=cmr12 at 14pt \font\rmXVIII=cmr17 at 18pt
\font\bfXVIII=cmbx18
\font\bfVIII=cmbx8 \font\slVIII=cmsl8

\input epsf % For Postscript figures: {\epsfxsize=  \epsffile{}}

\def\section#1#2{\par\vskip 17pt\leftline{\bf#1.\hskip 14truept#2}
\nobreak\smallskip\noindent\ignorespaces}
\let\xpar=\par
\def\smallfn#1#2{{\def\strut{\sstrut\xpar\kern2pt}\rmVIII\footnote
 {#1}{#2}}}
\def\R{\mathop{\rm I}\!\!\mathop{\rm R}\nolimits}
\nobreak
\def\caption#1#2{\parindent=8pt
  \narrower\narrower\noindent\hlvb Figure #1. #2\par}
\def\tabcaption#1#2{\parindent=20pt
  \narrower\narrower\noindent\hlvb Table #1. #2\par}
\def\heading#1#2{\par\vskip-\lastskip\bigskip
 \leftline{\bfXII #1 #2}\nobreak\medskip}
\def\sheading#1#2{\par\vskip-\lastskip\bigskip
 \leftline{\bfXI #1 #2}\nobreak\medskip}
\def\ssheading#1#2{\par\vskip-\lastskip\bigskip
  \noindent{\bf #1 #2. }\ignorespaces}
\def\lheading#1#2{\par\vskip-\lastskip\bigskip % > 1 line heading
  {\bfXII\setbox11=\hbox{#1 }\hangindent=\wd11 \hangafter=1
  \noindent #1 #2\par}\nobreak\medskip}
\def\lsheading#1#2{\par\vskip-\lastskip\bigskip % > 1 line subheading
  {\bfXI\setbox11=\hbox{#1 }\hangindent=\wd11 \hangafter=1
  \noindent #1 #2\par}\nobreak\medskip}
\def\bull{\item{$\bullet$}} \let\xpar=\par
\def\smallfn#1#2{{\def\strut{\xpar\kern2pt}\rmVIII \footnote {#1}{#2}}}
%
% Set up reference labels. Usage: \rn\Name in prologue and \Name\ in
%				  text instead of [1].
\newcount\refNUM \refNUM=0
\def\rn#1{\advance\refNUM by 1
	\edef#1{\hbox{[\number\refNUM]}}}

\rn \AD
\rn \ASZ
\rn \ASZI
\rn \C
\rn \D
\rn \DPAM
\rn \DT
\rn \DWC
\rn \E
\rn \GF
\rn \GSG
\rn \HWRSVJBT
\rn \JPS
\rn \KWT
\rn \RKW
\rn \TZWBIB
\rn \VL
\rn \WCSF
\rn \WIHM

\headline={\hfil}
\footline={\hfil}

\centerline{\bfXIV Triobjective optimization Using an Adaptive Weighting Scheme}
\vskip 1.2pc
\centerline{\rmXII Shubhangi Deshpande{$^a$}, Layne T. Watson{$^{a,b}$
}, Robert A. Canfield$^c$}
\smallskip
\centerline{\rmVIII $^a$Department of Computer Science, Virginia Polytechnic 
Institute \& State University, Blacksburg, VA}
\centerline{\rmVIII $^b$Department of Mathematics, Virginia Polytechnic 
Institute \& State University, Blacksburg, VA}
\centerline{\rmVIII $^c$Department of Aerospace \& Ocean Engineering, Virginia 
Polytechnic Institute \& State University, Blacksburg, VA}
\bigskip

\noindent{\bf Abstract}
\smallskip
\noindent A new method is proposed for approximating a Pareto front of 
a bound constrained biobjective optimization problem (BOP) where the evaluation 
of the objective functions is very expensive and/or the structure of the 
objective functions either cannot be exploited or not known. The  
method employs a hybrid optimization approach using two direct search 
techniques (dividing rectangles and mesh adaptive direct search). The algorithm  
iteratively formulates and solves several single objective optimization 
problems of the original BOP by using an adaptive weighting scheme, and moves 
closer to the true Pareto front at the end of each iteration. The method is 
tested on problems from the literature designed to illustrate some of the 
inherent difficulties in biobjective optimization such as a nonconvex or 
disjoint Pareto front, local Pareto front, or a nonuniform Pareto front. 
Finally, the algorithm is compared with a recent biobjective optimization 
algorithm, BiMADS, that generates an approximation of the Pareto front by 
solving a series of single objective formulations of the original BOP. 
Results show that the proposed algorithm efficiently generates a set of evenly 
distributed globally Pareto optimal solutions for a diverse types of problems. 
The accuracy and the distribution of the solutions obtained can be improved 
further with a relaxed budget in terms of true function evaluations. 

\bigskip\noindent{\it Keywords:} multiobjective optimization; Pareto 
optimality; direct search method; DIRECT; MADS; surrogates; triangulation.
\smallskip
\hrule
\smallskip
%
% equation numbers at left, numbered within section.  Subsection numbers
% of form  a.b.c  .  Theorems, definitions, figures in SIAM journal format.
%

\section{1}{Introduction}% begin text without a blank line in between
Multiobjective optimization problems arise in practically every field of 
science where optimal decisions need to be taken in the presence 
of two or more conflicting objectives. There usually is no unique solution 
that simultaneously optimizes all objectives; the goal of a multiobjective 
optimization problem is to find the best trade-off solutions, popularly known 
as Pareto optimal solutions (and the set of all the Pareto optimal solutions 
as a Pareto front), among these conflicting objectives. 

Let $\R^n$ denote real $m$-dimensional Euclidean space. For vectors $x^{(1)}$, 
$x^{(2)}$ $\in \R^n$ write $x^{(1)} < x^{(2)}$ $(x^{(1)}\le x^{(2)})$ if 
$\forall i$, $x^{(1)}_i<x^{(2)}_i (x^{(1)}_i\le x^{(2)}_i)$, $i=0$, $\ldots$, 
$n$. Let $l$, $u \in \R^n$ with $l < u$ and $B = \{ x \in \R^n \mid l\le x\le 
u\}$. Given $m$ conflicting objective functions $f_i: B \to \R$, $i=1,\ldots,m$, 
the multiobjective optimization problem is to simultaneously minimize all the 
$f_i$ over $B$.

For a nontrivial multiobjective optimization problem with $m$ conflicting 
objectives no single solution can optimize all $m$ objectives simultaneously. 
Hence, a new notion of optimality, known as Pareto optimality, is generally used 
in the context of multiobjective optimization problems. Pareto optimality 
generally defined using a dominance relation is as follows: Given two decision 
vectors $x^{(1)}$ and $x^{(2)}$, $x^{(1)}$ is said to dominate $x^{(2)}$ if and 
only if $x^{(1)}$ is no worse than $x^{(2)}$ in all objectives, and $x^{(1)}$ 
is strictly better than $x^{(2)}$ in at least one objective. This is generally 
denoted as $x^{(1)}\prec x^{(2)}$.

A solution $x^*$ is a globally Pareto optimal solution if and only if there 
does not exist any $x$ in the design space such that $x\prec x^*$. 

A solution $x^*$ is a locally Pareto optimal solution if and only if there 
does not exist any $x$ in $\epsilon$-neighborhood $\bigl\{ x \mid\|x-x^*\| 
\le\epsilon \bigr\}\cap B$ of $x^*$ that dominates $x^*$, for some 
$\epsilon > 0$.  

In general, finding infinitely many solutions on a Pareto front is impossible, 
and finding even a large discrete subset is hard when the structure and/or 
the analytical form of the underlying objective function is not known or 
is very complex (e.g., blackbox simulation optimization). An approximation 
to the true Pareto front is obtained as an alternative in such situations. 
A new method is proposed in this paper to handle such black box simulation 
optimization problems with two objectives. The algorithm employs a hybrid 
approach using two direct search techniques --- a deterministic global search 
algorithm, DIRECT \JPS\ to deal with the global optimality and a local direct 
search method, MADS (mesh adaptive direct search) \AD\ to fine tune the 
potentially optimal regions returned by DIRECT and to speed up the convergence. 
The idea is to systematically move towards the true Pareto front at the end of 
every iteration by adaptively filling the gaps between the nondominated 
solutions obtained so far. The algorithm uses the scalarization approach of Ryu 
et al. \RKW\ to convert a biobjective optimization problem into a single 
objective optimization problem. The true function evaluations are replaced by 
cheaper surrogates to the objective functions in order to minimize the 
expensive simulation runs. The novel idea proposed in this paper, using the 
global search algorithm DIRECT as a sampling method, proved to be promising in 
higher dimensional design spaces (see Section 4.1.4). Results are compared 
with those using a recent biobjective optimization algorithm BiMADS \ASZI. 
Results show that the proposed method is efficient, provides an 
arbitrarily close approximation to the true Pareto front, and finds well 
distributed global Pareto optimal fronts for diverse types of problems. 

\section{2}{Background} 
Many different methods have been suggested in the literature for solving 
multiobjective optimization problems. The solution approaches can be divided 
into two broad categories --- evolutionary approaches and scalarization or 
aggregation approaches. An evolutionary approach (e.g., \DPAM) yields a set 
of candidate Pareto solutions at the end of each iteration, but  requires a 
large number of true function evaluations and does not guarantee optimality in 
general. Aggregate or scalarization approaches \E\ are considered to be 
classical methods for solving multiobjective optimization problems. The 
general idea is to combine all the objectives to convert the multiobjective 
optimization problem into a single objective optimization problem. Most 
scalarization methods assume some problem knowledge and require a set of 
predefined parameters. A scalarization method called BiMADS \ASZ\ solves a 
biobjective optimization problem through a series of single objective 
formulations using the direct search method MADS, and attempts to obtain a 
uniform coverage of the Pareto front, even in the case when the Pareto front 
is nonconvex or disjoint. Another method, PAWS \RKW, claims efficient 
generation of evenly distributed points for various types of Pareto fronts 
using an adaptive weighting scheme. However, both these algorithms have their 
own limitations. Results produced using BiMADS are dependent on the specific 
implementation of the algorithm (e.g., Latin hypercube sampling used for the 
very first run in NOMAD 3.5.1 may not produce consistent results always). PAWS 
has an efficient weighting scheme, however the central composite designs based 
sampling strategy used in PAWS is impractical in higher dimensional search 
domains. Also PAWS generates local Pareto optimal fronts. The Pareto 
approximation algorithm employs the adaptive weighting scheme of PAWS, but 
tries to overcome both the limitations of PAWS with a novel sampling strategy 
and a global search method. 


Several real world scientific and engineering applications require analysis of 
spatially distributed data from expensive experiments or complex computer 
simulations that can take hours to run even on a fast workstation. This makes 
it difficult to explore and produce all design combinations in large high 
dimensional design spaces. Identifying the regions that contain good designs in 
such data-scarce domains by keeping the number of simulation runs to 
a minimum is a nontrivial problem. The proposed algorithm employs 
a systematic way to enrich an incomplete database by adaptively 
filling the gaps between the nondominated points obtained from available data. 
The work in this paper deals with biobjective optimization problems for 
such black box simulations where the structure of the objective functions is 
not known or is very complex. To tackle the problem of expensive simulation 
runs and to reduce the number of true function evaluations, computationally 
cheap(er) approximations (known as surrogates) of the underlying complex 
functions are used in the proposed work. Statistical sampling is a crucial 
part of a surrogate building process which becomes nontrivial in higher 
dimensional search domains. A novel idea proposed in this paper is of using 
DIRECT as a sampling strategy instead of using traditional space filling 
techniques. 

Hybrid optimization is a popular approach in the optimization community where 
several different optimization techniques are combined to improve robustness 
and blend distinct strengths of different approaches \GF. This same 
notion has been extended to multiobjective optimization problems in \WIHM. 
The Pareto approximation method of this paper is a hybrid approach using two 
direct search methods, DIRECT (to explore the design space globally) and 
MADS (to fine tune the potentially optimal regions returned by DIRECT), to 
balance between global and local searches and to improve the convergence rate.

The preliminary results for the proposed work were presented and discussed 
in \DWC. 

\section{3}{The Triobjective Optimization Algorithm}
This paper proposes a new method for triobjective optimization of possibly 
nonsmooth functions. The ordering property (that the objective functions 
can be ordered in two dimensions) is exploited in the biobjective case to 
find the most isolated point from a set of nondominated solutions at each 
iteration. However, due to the lack of ordering in higher dimensions (more than 
two), the same strategy cannot be implemented in the triobjective 
case. Hence, an alternative using the concept of triangulation borrowed 
from computational geometry is proposed in this paper. Section 3.1 proposes 
this alternate approach, and the general scheme of the triobjective 
optimization algorithm is presented in the Section 3.2.

\section{3.1}{An alternative using Delaunay triangulation}
In the biobjective optimization case, a point with the largest Euclidean 
distance with its neighbors is selected as the most isolated point provided 
that the data points are ordered in either of the two objectives. As mentioned 
earlier, this ordering property is not available when $d>2$. Hence, a concept 
from computational geometry, known as triangulation, is used instead. 
Triangulation of a discrete set of points is a subdivision of the convex hull 
of the points into simplices (set of triangles). A frequently used and studied 
point set triangulation is the Delaunay triangulation. A Delaunay 
triangulation of a point set is a triangulation of the point set with the 
property that no point in the point set falls in the interior of the 
circumcircle of any triangle in the triangulation.


\section{3.2}{The optimization algorithm} 
As a preprocessing step, a global search is performed 
using DIRECT to explore the design space in unsampled regions of a database 
(if the database is empty, the global exploration step becomes the data 
generation step). DIRECT being an optimization algorithm, the design space 
exploration is conducted in an intelligent manner by focusing the 
global search in potentially optimal regions only. Using the data collected 
in the global search a surrogate is built over the entire design space to 
find individual optimum for each objective function using MADS. The candidate 
solutions are evaluated and stored in the database. A set $X^0$ of nondominated 
points (with respect to all database points) is obtained at the end of the 
preprocessing step that serves as an input or starting point for the iterative 
procedure of the algorithm that follows. 

Each iteration of the algorithm consists of three steps: finding the most 
isolated point from the current nondominated point set, constructing surrogates 
for the objective functions with the isolated point determined in Step 1 as 
the center, and solving several single objective optimization problems to 
produce candidate Pareto solutions. At the beginning of the iterative process, 
a  trust region radius $\Delta^0$, the trust region contraction parameter 
$\rho$, and the tolerance value $\tau$ for the trust region radius are 
initialized to user defined values. \smallskip



\noindent{\bf Step 1: Isolated point determination}\smallskip

\noindent Let $X^{(k)} = \bigl\{x^{(k,1)}$,$\ldots$,$x^{(k,J_k)}\bigr\}$ be the 
set of nondominated points at the start of the iteration $k$ and 
$P^{(k)} = \bigl\{F(x^{(k,1)})$, $F(x^{(k,2)})$,$\ldots$,$F(x^{(k,J_k)})\bigr
\}$ be the corresponding objective space where $F(x) = \bigl (f_1(x)$, 
$f_2(x)\bigr )$ and $J_{k}$ is the cardinality of $X^{(k)}$ (and $P^{(k)}$). 
Note that for the very first iteration ($k=0$) the nondominated set $X^{(0)}$ 
is the result of the preprocessing step. Both $X^{(k)}$ and $P^{(k)}$ are 
sorted in descending order of the $f_1$ value. The gap between the nondominated 
points in the objective space $P^{(k)}$ is computed for each point in $P^{(k)}$ 
except for the end points as follows, for $j=2$, $3$, $\ldots$, $J_{k}-1$, 
$\delta_j^k=\|F(x^{(k,j-1)})-F(x^{(k,j)})\|_2 + \|F(x^{(k,j)})-F(x^{(k,j+1)})
\|_2$. The point $x_c^{(k)}$ with the largest $\delta_j^k$ 
value is selected as the most isolated point, and is used as the center point 
for a local model with a trust region radius $\Delta^k$. Thus,

\noindent if $J_k > 2$, $x_c^{(k)} := x^{(k,i)}$, where $i=$\hbox{argmax}
$[\delta_j^k]$, $j=2$, $\ldots$, $J-1$,

\noindent if $J_k = 2$,  $x_c^{(k)} := x^{(k,1)}$ or $x^{(k,2)}$ (select 
randomly),

\noindent if $J_k = 1$, $x_c^{(k)} := x^{(k,1)}$.

At the end of each iteration the point $x_c^{(k)}$ and the trust region radius 
$\Delta^k$ for the iteration $k$ are stored in a database. If $x_c^{(k)}$ 
happens to be a previously visited point (for $k > 0$) i.e., if $\exists 
i\in\{1$, $2$, $\ldots$, $k-1\}$, such that $x_c^{(k)} = x_c^{(i)}$, the trust 
region radius is modified as $\Delta^k := \rho\Delta^i$, and if the new 
$\Delta^k$ value is greater than $\tau$, $x_c^{(k)}$ is the center point for 
the local model for the next iteration; otherwise $x_c^{(k)}$ is accepted as a 
Pareto optimal point (and excluded from further consideration as an isolated 
point) and the above procedure is repeated until a new isolated point is 
found.\smallskip

\noindent{\bf Step 2: Surrogates}\smallskip

\noindent The algorithm uses a linear Shepard method (LSHEP from 
\TZWBIB) to approximate a response surface within a trust region. The 
isolated point $x_c^{(k)}$ determined in the previous step serves as the center 
point for the local trust region with radius $\Delta^k$ taken as the 
intersection of the local trust region box $\bigl\{x\mid\|x-x_c^{(k)}\|_\infty 
\le\Delta^k \bigr\}$ with the box $[l,u]$ defined by the variable bounds $l\le 
x \le u$. A tuning parameter $\epsilon$ for DIRECT allows a user to control 
the exploration mode for DIRECT (local versus global). The number of points to 
be sampled can be specified as an input to DIRECT, as for a Latin hypercube, 
the difference being deterministic adaptive rather than random point placement. 
The points to be sampled using DIRECT are based on the value of the 
aggregate objective constructed as a convex combination of the two objective 
functions. At the end of a single run of DIRECT, a data set $D^{(k)}={\Bigl\{
\bigl (x, f_1(x), f_2(x)\bigr)\mid x\in X_d^{(k)} \Bigr\}}$ of design 
samples $x\in X_d^{(k)}$ and system analyses $f_1$, $f_2$ is generated. Two 
LSHEP \TZWBIB\ surrogates $\tilde f_1$ and $\tilde f_2$ are built for the two 
objective functions $f_1$ and $f_2$ using the database $D^{(k)}$ and all the 
sampled points from previous iterations. At every iteration, several single 
objective optimization problems are formulated using convex combinations of 
these surrogates. Two of the weight combinations used have the values of $0$ 
and $1$ for each iteration (to compute the individual optima of the two 
surrogates). One or two additional weights are derived using the adaptive 
weighting scheme of Ryu et al. \RKW. Based on the value of the objective 
function at the center point $x_c^{(k)}$ for an iteration and its two 
neighbors, at most two additional weights $w^p$ and $w^q$ are computed at 
every iteration as follows. Let $x^{(k,i)} := x_c^{(k)}$.
$$\hbox{If } J_k > 2, w^p=[w_1^p,w_2^p]=c^p\Bigl [\bigl ((-f_2(x^{(k,i)})+f_2(x^{(k,
i-1)})\bigr ), \bigl((f_1(x^{(k,i)})-f_1(x^{(k,i-1)})\bigr )\Bigr ]$$
$$w^q=[w_1^q,w_2^q]=c^q\Bigl [\bigl ((-f_2(x^{(k,i+1)})+f_2(x^{(k,
i)})\bigr ),\bigl((f_1(x^{(k,i+1)})-f_1(x^{(k,i)})\bigr )\Bigr ]$$
$$\hbox{if } J_k = 2, w^p=[w_1^p,w_2^p]=c^p\Bigl [\bigl ((-f_2(x^{(k,2)})+f_2(x^{(k,1)}) 
\bigr),\bigl (f_1(x^{(k,2)})-f_1(x^{(k,1)})\bigr )\Bigr ]$$
$$\hbox{if } J_k = 1, w^p = [w_1^p,w_2^p] = [0.5,0.5]$$

\noindent where $c^p$ and $c^q$ are constants leading to $w_1^p+w_2^p=1$ and 
$w_1^q+w_2^q=1$. Thus, the surrogates constructed at each iteration include 
\par\noindent $s_1 = \tilde f_1$,\quad $s_2=\tilde f_2$, \quad and
\par\noindent if $J^k > 2$,\quad 
$s_p = w_1^{p}s_1+w_2^{p}s_2$,\quad $s_q=w_1^{q}s_1+w_2^{q}s_2$,
\par\noindent if $J^k \le 2$,\quad  
$s_p = w_1^{p}s_1+w_2^{p}s_2$.\smallskip

\noindent{\bf Step 3: Solving optimization problems}\smallskip

\noindent Each of the surrogates constructed in Step 2 is optimized using MADS 
to get a candidate solution. Thus at the end of each iteration at most four 
candidate Pareto solutions are obtained, evaluated to get their true function 
values, and then nondominated solutions among those are selected. Let $X_*^{
(k)}$ be a set of these nondominated solutions. An updated set $X^{(k+1)}$ of 
nondominated points is obtained at the end of iteration $k$ by comparing all 
points from the  union $X^{(k)}\cup X_d^{(k)}\cup X_*^{(k)}$.

These three steps are iterated by the algorithm until a stopping 
condition is satisfied. In practice, termination is either set to a fixed 
number of iterations, or when the value of the star discrepancy (discussed 
in detail in the next section) drops below a predefined threshold.
\vskip -3pt
\section{4}{Numerical Evaluation}\vskip -3pt
\section{4.1}{Test Problems}
The biobjective optimization algorithm is evaluated using test 
problems from \D. Each test problem is intended to illustrate the 
capability of the algorithm to handle diverse types of Pareto 
fronts. The objective functions for these problems are constructed in such a 
way that

\noindent $x=\bigl (x_1$, $\ldots$, $x_m$, $\ldots$, $x_n\bigr )$,

\noindent $f_1(x) = f\bigl (x_1$, $\ldots$, $x_m\bigr )$,

\noindent $f_2(x) = g\bigl (x_{m+1}$, $\ldots$, $x_n)\cdot h\bigl(f(x_1$, 
$\ldots$, $x_m\bigr )$, $g(x_{m+1}$, $\ldots$, $x_n)\bigr )$. \smallskip
 
Difficulties in test problems are introduced by choosing appropriate functions 
$f$, $g$, and $h$ \D. The test problems that are considered here are defined 
through specific choices of $f$, $g$, and $h$.  

\section{4.1.1}{Problems 1 and 2: multimodal biobjective functions with 
convex and nonconvex front}
The test problems in this category have a multimodal objective 
function $f_2$ with two local minima and a unique global minimum. The 
convexity of the Pareto front is achieved by the function $h$ 
defined as
$$h(f,g)=\cases{1-{ { ( {f}/g )}^\alpha },&if ${f}\le g$;\cr 
0,&otherwise.\cr}$$
The parameter $\alpha$ controls the shape of the Pareto front (nonconvex if 
$\alpha>1$ and convex otherwise). $g$ and $f$ as defined in \RKW\ are 

$$g(x_2)=\cases{4 - 3\exp({-\left({x_2-0.2}\over{0.02}\right)^2}),&if $0\le 
x_2\le 0.4$;\cr 
4 - 2\exp({-\left({x_2-0.7}\over{0.2}\right)^2}),&if $0.4 < x_2\le 1$.\cr}$$

\noindent $f(x_1) = 4x_1$, where $x_1$,$x_2 \in [0$,$1]$. 

\noindent The values used in this paper for $\alpha$ are $0.25$ and $4$ for 
convex and nonconvex Pareto fronts, respectively.\smallskip

\section{4.1.2}{Problem 3: discontinuous Pareto front}
The functions for a discontinuous Pareto front as defined in \D\ 
follow.

\noindent $h(f$,$g) = 1 - {{(f/g)}^\alpha} - {(f/g)}\sin(2\pi qf)$,

\noindent $f(x_1) = x_1$,\qquad $g(x_2) = 1 + 10x_2$,

\noindent where $x_1$, $x_2\in [0,1]$, $\alpha>0$, and $q$ is the number of 
discontinuous regions. The values used in this paper are $\alpha = 2$ and 
$q = 4$.

\section{4.1.3}{Problem 4: nonuniformly represented Pareto front}
A choice of a nonlinear function for $f$ introduces nonuniformity of 
the solution set along the Pareto front. The function $f$ as defined in \D\ is 
$$f(x_1) = 1 - \exp(-4x_1)\sin(5\pi x_1)^{4},$$ 
$x_1$, $x_2\in [0,1]$. The functions $h$ and $g$ are the same as that for the 
problem with a nonconvex Pareto front from Section 4.1.1. Deb \D\ shows that 
for a uniformly spaced points in $x_1$, solutions cluster around $f_1 = 1$, 
and hence would be a good test for evaluating the method. 

\section{4.1.4}{\bf Problem 5: high dimensional search space}
The way the functions $f$, $g$, and $h$ are proposed by Deb \D\ facilitates 
an easy extension to a problem with higher dimensional search domain. The 
following definitions are considered for $f$, $g$, and $h$ to formulate a 
test problem with twenty design variables.

\noindent $f(x_1) = x_1$,

\noindent $g(x_2$,$\cdots$,$x_{20}) = 1 + 10*19 + \sum_{i=2}^{20}{x_i^2-10
\cos(4\pi x_i)}$,

\noindent $h(f$,$g) = 1-{\bigl(x_1/g\bigr)}^2$, where $x_1\in [0,1]$ and 
$x_2 \in [-5,5]$.

Results for all the test problems are presented and discussed in Section $4.5$.

\section{4.2}{Comparison with BiMADS}
A scalarization method called BiMADS \ASZ\ solves a biobjective optimization 
problem through a series of single objective formulations using MADS, and 
attempts to obtain a uniform coverage of the Pareto front, even in the case 
when the Pareto front is nonconvex or disjoint. Results for the test problems 
of 4.1 are compared with the results using BiMADS. For the comparison, the 
C++ implementation of BiMADS available with the software package NOMAD (version 
3.5.1) \DT\ was used. For each test problem the starting point was set to the 
center of the design space. The algorithm was terminated using the stopping 
condition of the total budget in terms of the number of blackbox evaluations 
(MULTI\_OVERALL\_BB\_EVAL). 

\section{4.3}{Parameter setup}
For the numerical comparisons here the parameter MULTI\_OVERALL\_BB\_EVAL 
was set to one thousand evaluations for all the problems.

The proposed method uses a Fortran 95 implementation VTDIRECT \HWRSVJBT\ of the 
algorithm DIRECT and a C++ implementation NOMAD (version 3.5.1) of the 
algorithm MADS.
 
The initial parameter setup for VTDIRECT for the preprocessing step follows. 
The design variables lower bound, $LB := [l_1$, $\ldots$, $l_n]$, the design 
variables upper bound, $UB := [u_1$, $\ldots$, $u_n]$, and $\epsilon := 0.1$, 
where $\epsilon$ is a tuning parameter that controls the exploration mode of 
DIRECT (local versus global). The upper limit on the number of true function 
evaluations, $eval\_lim$ was set to two hundred runs for the problems in  
4.1 and 4.4, to five hundred runs for the problem in 4.3, and to twenty five 
for the problem in 4.2. The extent to which the global search was performed 
was based on the nature of the test problem under consideration (conservative 
for simple problems and extensive for difficult ones like with multimodal 
objectives or high dimensional design space). VTDIRECT is run twice with this 
setup for two single objective optimization problems of the form, 
$(1-w)f_1+wf_2$ where $w$ is set to $0$ for the first run and to $1$ for the 
second run. If a point already exists in the database within a tolerable 
distance ($1$E$-13$) from the current sampled point, then the point in the 
database is used instead of repeating the system analysis at the sampled point. 

The parameter setting for MADS and VTDIRECT for the main iterative procedure 
follows. MADS: the starting point $x^{(0)}$ is set to the center point 
determined for that iteration, the bound constraints for the MADS run are set to 
the local model boundaries, the initial mesh size is set to $0.01$, and 
stopping criteria are set for the minimum mesh size at $0.0001$ and the maximum 
number of true function evaluations at $50$. VTDIRECT: $LB$ and $UB$ are set to 
the local trust region boundaries for that iteration, $eval\_lim := 2n+3$ where 
$n$ is the number of design variables, and $\epsilon := 0.1$. The single 
objective formulation is the same as the one used in the preprocessing step. 
The first $2n+1$ sampling locations using DIRECT are always the same (the 
center point $c$ of the local model and $c\pm\delta e^{(i)}$, $i=1$,$\ldots$, 
$n$ for the $n$ coordinate directions for the problems with $n$ design 
variables. The locations for the last two samples depend on the aggregate 
function values after the single objective formulation. Hence for the 
second run of VTDIRECT ($w=1$) the first five locations are the same as those 
for the first run ($w=0$), and the last two sampling locations could differ 
from those of the first run based on the aggregate function values at the first 
five locations. Thus at the end of the sampling process (two runs of VTDIRECT 
for the weights 0 and 1) seven to nine samples are generated at each iteration. 
The local trust region parameters are set as $\Delta := 0.2$, $\rho := 0.5$, 
and $\tau := 0.001$.

\section{4.4}{Performance Measures}\vskip -3pt
Four performance measures used to evaluate the algorithm are 
accuracy (closeness to the true Pareto front) $GD$, dispersion (distribution of 
the obtained Pareto optimal set) $D^*$, efficiency (number of true 
function evaluations) $N_{FE}$, and representativeness (number of unique Pareto 
optimal solutions obtained) $N_{PS}$.

\topinsert
\vbox{
\line{\epsfxsize=2.9truein\epsffile{conresl.eps}\hfil\epsfxsize=2.9truein
\epsffile{myconres.eps}}
{\rmIX Figure 1. (a) BiMADS and (b) the proposed method with a budget of $1000$ 
(maximum) evaluations each for the problem 4.1.1 (convex Pareto front, circles
).}
}
\endinsert

To measure the accuracy of the obtained Pareto optimal solutions, generational 
distance ($GD$) measure introduced in \VL\ is used. $GD=0$ indicates that all 
the obtained solutions belong to the true Pareto front. A uniform $500\times  
{500}$ grid of closely spaced points is evaluated to obtain a set $H$ of true 
Pareto optimal solutions. The set $H$ is used to compute the generational 
distance for each test problem. The minimum Euclidean distance $\hat\delta$ of 
each solution (from the final Pareto optimal set obtained for each test 
problem) from the elements in $H$ is computed, and then $GD$ is computed as 
follows.
$$GD = \sqrt{{\sum_{i=1}^n {\hat\delta}^2}\over n}$$
\noindent where, $n$ is the cardinality of $X$.

\topinsert
\vbox{
\line{\epsfxsize=2.9truein\epsffile{nconresl.eps}\hfil\epsfxsize=2.9truein
\epsffile{mynconres.eps}}
{\rmIX Figure 2. (a) BiMADS and (b) the proposed method with a budget of $1000$ 
(maximum) evaluations each for the problem 4.1.1 (nonconvex Pareto front, 
circles).}
}\endinsert

A precise mathematical measure, star discrepancy, is introduced in this paper 
for assessing the distribution of the obtained Pareto optimal solutions. 
General discrepancy \KWT\ of a sequence $S_N = \{s_i^N\}$, where $i=1$, 
$\ldots$, $N$ and $s_i\in {[0,1]}^d$ is defined as 
$$D(\beta;S_N) = {\sup_{B\in\beta} \mid{{A(B;S_N)}\over N} - \lambda_d(B) \mid 
},$$ 
\noindent where $\beta$ denotes a family of Lebesgue measurable subsets of 
${[0,1]}^d$, $A(B$;$S_N)$ is the number of points from $S_N$ that are also 
elements of $B$, and $\lambda_d(B)$ is the $d$-dimensional Lebesgue measure of 
$B$. For practical purposes, a variation of general discrepancy, star 
discrepancy \C, \KWT, is often used. Let $J^*$ denote the family of Lebesgue 
measurable subsets of ${[0,1]^d}$ of the form $\prod_{i=1}^d [0,v_i)$. Then 
the star discrepancy of the sequence $S_N$ is defined as
$$D_N^*(S_N) = D(J^*;S_N).$$
A family of sequences ${\{S_N\}}^\infty_{N=1}$ is said to be 
uniformly distributed if and only if
$$\lim_{N\to\infty}{D_N^*(S_N)} = 0.$$
\noindent Typically, Pareto fronts in biobjective optimization problems are 
one-dimensional manifolds, and the length of a Pareto curve constitutes its 
volume. For each test problem, the length of the resulting Pareto front is 
approximated by adding up the pairwise Euclidean distances $\bar\delta_j$ for 
all points $x_i$ in the Pareto optimal set in descending order of $f_1$ 
values,
$$\sum_{1}^{J-1}{\bar\delta_j = \|F(x_j)-F(x_{j+1})\|_2},$$ 
\noindent where $J$ is the cardinality of the Pareto optimal set, and then 
normalized to unit length. Each $s_i$ indicates the normalized piecewise linear
distance of each point in the Pareto optimal set (in descending order of $f_1$ 
value) from the $x_1$. The star discrepancy is computed using $J^*$ of the 
form $[0,s_i)$. 
The next subsection presents the Pareto optimal fronts for the five test 
problems along with all four performance measures ($GD$, $D^*$, $N_{FE}$, and 
$N_{PS}$). 

\vskip -3pt
\section{4.5}{Results and Discussion}\vskip -3pt
\topinsert
\vbox{
\line{\epsfxsize=2.9truein\epsffile{discres.eps}\hfil\epsfxsize=2.9truein
\epsffile{mydiscres.eps}}
{\rmIX Figure 3. (a) BiMADS and (b) the proposed method with a budget of  
$1000$ (maximum) evaluations each for the problem 4.1.2 (Pareto optimal 
points as circles).}
}
\endinsert

\topinsert
\vbox{
\line{\epsfxsize=2.9truein\epsffile{nuformresl.eps}\hfil\epsfxsize=2.9truein
\epsffile{mynuformres.eps}}
{\rmIX Figure 4. (a) BiMADS and (b) the proposed method with a budget of $1000$ 
(maximum) evaluations each for the problem 4.1.3(Pareto optimal 
points as circles).}
}
\endinsert

\topinsert
\vbox{
\line{\epsfxsize=2.9truein\epsffile{hdres.eps}\hfil\epsfxsize=2.9truein
\epsffile{myhdres.eps}}
{\rmIX Figure 5. (a) BiMADS and (b) the proposed method with a budget of 
$1000$  (maximum) evaluations each for the problem 4.1.4.(Pareto optimal 
points as circles)}
}
\endinsert

Figures $1b$---$5b$  and the Table $2$ show that the algorithm successfully 
finds a fairly good approximation ($GD$ values in Table $2$) to the 
global Pareto front with a well distributed ($D^*_N$ values in Table $2$) 
Pareto optimal set for diverse types of biobjective optimization problems. 
The  method presented in this paper using the adaptive weighting scheme of 
Ryu et al.\RKW\ and the guided sampling using DIRECT effectively approximates a 
Pareto front by gradually moving towards the true Pareto front by filling the 
gaps in the incumbent nondominated point set. In that sense it gradually learns 
the shape of the true Pareto front and concentrates its computational effort in 
potentially optimal regions. Both the quality and the quantity of the obtained 
Pareto optimal set improves with the increase in the number of iterations of 
the algorithm. The distribution of the global (preprocessing) and the 
local search in the course of the algorithm plays an important role in 
directing the results towards global Pareto fronts. However, it is a trade off 
between the number of true function evaluations ($N_{FE}$) and the accuracy 
($GD$). 

\topinsert
\line{\hfil
\vbox{\hsize 15truepc
\centerline{\caps {\bf Table $1$}}
\centerline{\sl Performance measures: BiMADS}
\smallskip
\centerline{\vbox{
\offinterlineskip \tabskip=0pt
\def\filler{height2pt&\omit&& \omit&&\omit&&\omit&&\omit&\cr}
\def\tablerule{\noalign{\hrule}}
\halign{&\vrule#&\strut\hskip 4pt\hfil#\hskip 4pt\cr
\tablerule\filler
&\hfill &&$GD$\hfil&&$D^*_N$\hfil&&$N_{FE}$\hfil&&$N_{PS}$\hfil&\cr
\filler\tablerule\filler
&Problem 1\hfill&&0.4475&&0.1045&&1000&&260&\cr
&Problem 2\hfill&&0.9766&&0.4736&&1000&&371&\cr
&Problem 3\hfill&&0.0017&&---&&1000&&178&\cr
&Problem 4\hfill&&0.8693&&0.3814&&1000&&276&\cr
&Problem 5\hfill&&0.0000&&0.0888&&1000&&35&\cr
\filler\tablerule}}}
}\hfil
\vbox{\hsize 15truepc
\centerline{\caps {\bf Table $2$}}
\centerline{\sl Performance measures: proposed method}
\smallskip
\centerline{\vbox{
\offinterlineskip \tabskip=0pt
\def\filler{height2pt&\omit&& \omit&&\omit&&\omit&&\omit&\cr}
\def\tablerule{\noalign{\hrule}}
\halign{&\vrule#&\strut\hskip 4pt\hfil#\hskip 4pt\cr
\tablerule\filler
&\hfill &&$GD$\hfil&&$D^*_N$\hfil&&$N_{FE}$\hfil&&$N_{PS}$\hfil&\cr
\filler\tablerule\filler
&Problem 1\hfill&&0.0029&&0.1477&&998&&144&\cr
&Problem 2\hfill&&0.0024&&0.0411&&994&&114&\cr
&Problem 3\hfill&&0.0022&&---&&999&&129&\cr
&Problem 4\hfill&&0.0565&&0.0609&&1000&&71&\cr
&Problem 5\hfill&&0.0018&&0.3517&&967&&52&\cr
\filler\tablerule}}}
}
\hfil
}
\endinsert

While performing the numerical experiments it was observed that the behavior of 
BiMADS is sensitive to the parameter MULTI\_OVERALL\_BB\_EVAL used as the 
stopping condition. This is due to the way the algorithm BiMADS is implemented 
in NOMAD package --- when using the MULTI\_OVERALL\_BB\_EVAL parameter, it 
allows twice as many evaluations on the first run than on the others. The first 
run starts by performing a latin hypercube sampling. This benefits all the 
subsequent runs as it generates many points in the space of variables. However, 
due to this setup the maximum number of MADS runs for each iteration of BiMADS 
changes as the parameter MULTI\_OVERALL\_BB\_EVAL changes, and hence may not 
produce consistent results. This could be observed by looking at the local 
Pareto fronts obtained by BiMADS for the problems 4.1 and 4.3 (Fig. 1, 2, and 
4) and fairly uniformly distributed global Pareto fronts for the problems 
4.2 and 4.4 (Fig. 3 and 5). 

Tables $1$ and $2$ show the four performance measures---$GD$, $D_N^*$, 
$N_{FE}$, and $N_{PS}$---for the five problems in 4.1 using BiMADS and the 
proposed method respectively. Computing the star discrepancy for a 
discontinuous Pareto front is rather difficult and has not been considered in 
the current implementation. 

The most expensive step in the surrogate building process is the statistical 
sampling used to generate a database for a local approximation at every 
iteration of the algorithm. The major difficulty with these traditional 
techniques is the number of design points required in high dimensional search 
domains. For example, CCD based PAWS scheme would require a total of 
$2^{20}+2*20+1 = 1048617$ design points at every iteration for the problem 
4.1.4 with $20$ design variables. The novel idea presented in this paper of  
using the algorithm DIRECT as a sampling strategy alleviates this problem as 
it needs only $2n+1=41$ design points at each iteration. The advantage of using 
DIRECT over traditional sampling techniques is that DIRECT adaptively samples 
at box centers rather than at the corners ($2^n$ factor for CCD). That makes 
the number of design points linear in the number of design variables using 
DIRECT. 

\section{5}{Conclusion and Future Work}
A new Pareto front approximation algorithm for biobjective optimization 
problems was proposed and applied to five test problems from the literature 
designed to illustrate some inherent difficulties of biobjective optimization 
problems. The algorithm was evaluated by studying four performance measures, 
generational distance (proximity to the true Pareto front), star discrepancy 
(distribution of the Pareto optimal set), the number of unique Pareto optimal 
solutions found, and the number of true function evaluations, and results 
were compared with those using a biobjective optimization algorithm BiMADS. 
Results show that the  algorithm is efficient, offers an arbitrarily close 
approximation to the true Pareto front, and finds global Pareto optimal 
solutions for variety of problems with diverse types of fronts. An extension of 
the current work would be for problems with more than two objectives, which 
requires revisiting the current adaptive weighting scheme for more than two 
objectives and also a new approach to compute the volume of Pareto front 
manifolds in higher dimensions for the star discrepancy.

\bigskip
\noindent{\bf References}
\newbox\refbox
\setbox\refbox=\hbox{\rmIX [55] }\newcount\refnum \refnum=0
\def\refb#1#2#3{{\noindent\hangindent=\wd\refbox\hangafter=1 % book
\global\advance\refnum by 1
\rmIX\hbox to \wd\refbox{\hss[\number\refnum] }#1,
{\slIX #2}, #3\par}}
\def\refj#1#2#3#4{{\noindent\hangindent=\wd\refbox\hangafter=1 % journal
\global\advance\refnum by 1
\rmIX\hbox to \wd\refbox{\hss[\number\refnum] }#1, ``#2,''
{\slIX #3}, #4\par}}
\def\reftr#1#2#3{{\noindent\hangindent=\wd\refbox\hangafter=1 % tech rep
\global\advance\refnum by 1
\rmIX\hbox to \wd\refbox{\hss[\number\refnum] }#1, ``#2,'' #3\par}}
\def\refp#1#2#3#4{{\noindent\hangindent=\wd\refbox\hangafter=1 % proc
\global\advance\refnum by 1
\rmIX\hbox to \wd\refbox{\hss[\number\refnum] }#1, ``#2,'' in
{\slIX #3}, #4\par}}
\def\lrule{\leaders\hrule\hskip 2em}\frenchspacing
%
% begin references without a blank line.
\smallskip
\refj %AD
{Audet C. and Dennis J. E.}{Mesh adaptive direct search algorithms for 
constrained optimization}{SIAM Journal on Optimization}{Vol 17, No. 1, 
pp. 188-217, 2006}

\refj %ASZ
{Audet C., Savard G., and Zghal W.}{A mesh adaptive direct search algorithm 
for multiobjective optimization}{European Journal of Operational Research}
{Vol. 204, pp. 545-556, 2010}

\refj %ASZI
{Audet C., Savard G., and Zghal W.}{Multiobjective optimization through a 
series of single objective formulations}{SIAM Journal of Optimization}
{Vol. 19, pp. 188-210, 2008}

\refj %C
{Caflisch R. E.}{Monte Carlo and quasi-Monte Carlo methods}{Acta Numerica}{vol. 
7, pp. 1--49, 1998}

\refj %D
{Deb K.}{Multiobjective genetic algorithms: problem difficulties and 
construction of test problems}{Evol. Comput.}{Vol. 7. pp. 205---230, 1999}

\refj %DPAM
{Deb, K., Pratap. A, Agarwal, S., and Meyarivan, T.}{A fast and elitist 
multiobjective genetic algorithm: NSGA-II}{IEEE Transaction on Evolutionary 
Computation}{6(2), pp. 181-197, 2002}

\refb %DT
{Digabel S. L. and Tribes C.}{NOMAD user guide version 3.5.1}{Mar, 2012}

\refp %DWC
{Deshpande S. G., Watson L. T., and Canfield R. A.}{Biobjective Optimization 
using Direct Search Techniques }{IEEE SoutheastCon}{Jacksonville, FL, Apr 4-7, 
2013}

\refb %E
{Eichfelder G.}{Adaptive Scalarization Methods in Multiobjective Optimization 
(Vector Optimization)}{Springer, 2008}

\refj %GF
{Gray G. A. and Fowler K. R. (2011)}{Traditional and hybrid derivative-free 
optimization approaches for black box functions}{Computational Optimization, 
Methods and Algorithms}{356}{125-151}

\refj %HWRSVJBT
{He J., Watson L. T., Ramakrishnan N., Shaffer C. A., Verstak A., Jiang J., 
Bae K., and Tranter W. H.}{Dynamic data structures for a direct search 
algorithm}{Computational Optimization and Applications}{Vol. 23, pp. 5--25, 
2002} 

\refj %JPS
{Jones D. R., Perttunen  C. D., and Stuckman B. E.}{Lipschitzian optimization
without the Lipschitz constant}{Journal of Optimization Theory and Application} 
{Vol. 79, No. 1, pp. 157-181, 1993}

\refp %KWT
{Kugele S. C., Watson L. T., and Trosset M. W.}{Multidimensional numerical 
integration for robust design optimization}{ACM Southeast Regional Conference}
{pp. 385-390, 2007}

\refp %RKW
{Ryu J., Kim S., and Wan H.}{Pareto front approximation with adaptive weighted 
sum method in multiobjective simulation optimization}{Proceedings of the 2009
Winter Simulation Conference}{pp. 623-633, Austin, TX, USA, 2009}

\refj %TZWBIB
{Thacker W. I., Zhang J, Watson L. T., Birch J. B., Iyer M. A., Barry M. W.}
{Algorithm 905: SHEPPACK: modified Shepard algorithm for interpolation of 
scattered multivariate data}{ACM Trans. Math. Software}{vol. 37, pp. 1-20, 
2010}

\refb %VL
{Veldhuizen D. A. and Lamont G. B}{Evolutionary Computation and 
Convergence to a Pareto Front}{Stanford University Bookstore, 1998}

\refp %WIHM
{Wang L., Ishida H., Hiroyashu T., and Miki M.}{Examination of 
multiobjective optimization method for global search using DIRECT and GA}
{IEEE World Congress on Computational Intelligence}{pp 2446--2451, 2008}

\bye
