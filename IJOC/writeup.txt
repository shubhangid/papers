Introduction:
Multiobjective optimization problem is an optimization problem that involves 
two or more conflicting objective functions that need to be optimized 
simultaneously. MOPs arise in practically every domain of science and 
engineering where optimal decisions need to be taken in the presence of 
two or more conflicting objectives. For example, in finance domain, a common 
problem is to choose a portfolio by optimizing two conflicting objectives 
risk and returns. 

For a nontrivial multiobjective optimization problem, there generally is no 
unique solution that optimizes all the objectives simultaneously, instead a 
set of trade off solutions is obtained. The goal of a MOP is to find the best 
trade-off solutions, popularly known as Pareto optimal solutions (and the 
set of all the Pareto optimal solutions as a Pareto front), among all the 
conflicting objectives.

Finding all the Pareto optimal solutions for a MOP is impossible; finding even 
a large discrete set is hard when the structure of objs is not known or is very 
expensive. 

definitions
Dominance relation
global and local Pareto optimality

what kind of problems we want to solve
global Pareto fronts, well distributed, close approximation to true fronts, 
good representation of the Pareto front

Organization of the paper


Background:
Applying a Pareto based ranking scheme using genetic algorithms has become 
very common and popular in most of the evolutionary approaches (e.g., \DPAM,), 
although some schemes based on particle swarm intelligence and simulated 
annealing are significant too. An evolutionary approach yields a set of Pareto 
solutions at the end of each iteration, however, requires a huge number of 
true function evaluations and does not guarantee optimality in general. 

Aggregate or scalarization approaches \E are considered 
to be classical methods for solving multiobjective optimization problems. 
A general idea is to combine all objective functions to convert a 
multiobjective optimization problem into a single objective optimization 
problem. Thus at the end of each iteration it yields a single solution by 
solving a single objective optimization problem. Among the commonly used 
classical methods for handling multiobjective optimization problems are 
weighted sum method [ref] (scalarizing a set of objectives 
into a single objective using a predefined weight vector), $\epsilon$-
constraint method [ref] (reformulating a multiobjective optimization problem by 
keeping one of the objectives and contraining the rest of the objectives using 
a set of user defined values), goal programming method [ref] (finding solutions 
that attain a predefined target for one or more objectives), and so on. 
Other methods like normal boundary intersection[], successive Pareto 
optimization[], directed search domain[] solve multiobjective optimization 
problems by constructing several aggregate objective functions. The major 
difficulty in these classical approaches is that not all Pareto optimal 
solutions can be found, especially in nonconvex multiobjective optimization 
problems. Also, most of the scalarization methods assume some problem knowledge 
and require a set of predefined parameters.

1. Special requirements in tri(multi)objective case
Biobjective case --- BiMADS, PAWS, and our previous work.
Limitations.
Extending to multiobjective case needs special attention. The ordering property 
available in biobjective case cannot be applied in multiobjective due to the 
lack of ordering in vectors. 
Computational geometry approach applied in the current work. 
1.1 Triangulation

2.
Surrogates for expensive simulations. 

3.
Hybrid optimization using two direct search methods.


The algorithm:
Preprocessing
Main iterations
Step 1: Isolated point determination
Step 2: 
Step 3: Optimization
Step 4: Parameter update

Box schematic of the algorithm

Convergence analysis



